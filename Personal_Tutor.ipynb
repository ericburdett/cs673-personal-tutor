{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personal-Tutor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiI4oMDnmlpkfswtrLfFa4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs673-personal-tutor/blob/master/Personal_Tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngPwFJK2Q3is",
        "colab_type": "text"
      },
      "source": [
        "# Personal Tutor\n",
        "\n",
        "This notebook contains code for the Personal Tutor System built for CS673: Computational Creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzpCKzikWxA3",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn2-Iv5RGY_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gpt-2-simple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUN5YCJsWzAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iLmsxoZGjSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a few different corpuses to work with GPT2\n",
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "!tar -xvf text_files.tar.gz\n",
        "!rm text_files.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg7SoAGkGh3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the children's book corpus from GitHub\n",
        "!wget -O cbt.txt https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/cbt_train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_JZMaHVEOg",
        "colab_type": "text"
      },
      "source": [
        "## Word Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx7LXT0sVLjI",
        "colab_type": "code",
        "outputId": "67c9dff3-f0f3-426b-a59d-951db417b83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download the simple word distribution from GitHub\n",
        "!wget -O word_dist_full.csv https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 21:26:15--  https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163042 (159K) [text/plain]\n",
            "Saving to: ‘word_dist_full.csv’\n",
            "\n",
            "\rword_dist_full.csv    0%[                    ]       0  --.-KB/s               \rword_dist_full.csv  100%[===================>] 159.22K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-04 21:26:16 (5.21 MB/s) - ‘word_dist_full.csv’ saved [163042/163042]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWVYaEVrV065",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordDist(Dataset):\n",
        "  def __init__(self):\n",
        "    self.df = pd.read_csv('word_dist_full.csv', header=None, names=['word', 'freq'])\n",
        "  \n",
        "  def getdf(self):\n",
        "    return self.df\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.df['word'][index], self.df['freq'][index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP2HehFTXIIN",
        "colab_type": "code",
        "outputId": "2595c88e-c462-4bbc-9cfc-8cba0f474152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "words = WordDist()\n",
        "print('Num Words: ', words)\n",
        "words[0:20]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Words:  <__main__.WordDist object at 0x7fb77d77a710>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0      the\n",
              " 1       of\n",
              " 2      and\n",
              " 3       to\n",
              " 4        a\n",
              " 5       in\n",
              " 6      for\n",
              " 7       is\n",
              " 8       on\n",
              " 9     that\n",
              " 10      by\n",
              " 11    this\n",
              " 12    with\n",
              " 13       i\n",
              " 14     you\n",
              " 15      it\n",
              " 16     not\n",
              " 17      or\n",
              " 18      be\n",
              " 19     are\n",
              " Name: word, dtype: object, 0     23135851162\n",
              " 1     13151942776\n",
              " 2     12997637966\n",
              " 3     12136980858\n",
              " 4      9081174698\n",
              " 5      8469404971\n",
              " 6      5933321709\n",
              " 7      4705743816\n",
              " 8      3750423199\n",
              " 9      3400031103\n",
              " 10     3350048871\n",
              " 11     3228469771\n",
              " 12     3183110675\n",
              " 13     3086225277\n",
              " 14     2996181025\n",
              " 15     2813163874\n",
              " 16     2633487141\n",
              " 17     2590739907\n",
              " 18     2398724162\n",
              " 19     2393614870\n",
              " Name: freq, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9LYBu6EVIkA",
        "colab_type": "text"
      },
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuQNroOuIg1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class that deals with training and generating text from GPT2\n",
        "class LanguageModel():\n",
        "  def __init__(self, model='124M', genre='children', train_steps=200):\n",
        "    self.download_model(model)\n",
        "    self.genre = genre\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    self.sess = gpt2.start_tf_sess()\n",
        "\n",
        "    if genre == 'children':\n",
        "      gpt2.finetune(self.sess, 'cbt.txt', model_name=model, steps=train_steps)\n",
        "    else:\n",
        "      raise('The specified genre does not exist')\n",
        "\n",
        "  # Returns a list of sample texts with a given prefix and suffix\n",
        "  def generate_text(self, prefix='<|startoftext|>', suffix='.', include_prefix=False, nsamples=5):\n",
        "    if nsamples < 1 or nsamples > 20:\n",
        "      raise('Error: nsamples must be within the range 1 <= x <= 20')\n",
        "\n",
        "    return gpt2.generate(self.sess, prefix=prefix, truncate=suffix, include_prefix=include_prefix, batch_size=nsamples, nsamples=nsamples, return_as_list=True)\n",
        "  \n",
        "  def download_model(self, model_name):\n",
        "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "      print(\"Downloading {model_name} model...\")\n",
        "      gpt2.download_gpt2(model_name=model_name)\n",
        "    else:\n",
        "      print(model_name, \" model is already downloaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQxMeo2JNzSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A class that contains some knowledge that the user has acquired over time.\n",
        "# For example, it may hold the words that the user knows (and how well the user knows them)\n",
        "class UserKnowledge():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # We will likely need some place to store the knowledge we acquire about the user\n",
        "  # so that we can access it from session to session\n",
        "  def save_knowledge(self, path):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK1wDhdgJajQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class that evaluates sentences based on what the system knows about the user\n",
        "class SentenceEvaluator():\n",
        "  def __init__(self, level='beginner', user_knowledge=None):\n",
        "    self.level = 'beginner'\n",
        "    if user_knowledge == None:\n",
        "      self.user_knowledge = UserKnowledge()\n",
        "    else:\n",
        "      self.user_knowledge = user_knowledge\n",
        "\n",
        "  # We will likely want some method to update the user_knowledge in the evaluator\n",
        "  # Maybe, we will only pass user_knowledge into the evaluate function?...\n",
        "  def update_user_knowledge(user_knowledge):\n",
        "    pass\n",
        "\n",
        "  # Score the sentences and return the sentence with the highest score\n",
        "  def evaluate(self, sentences):\n",
        "    scores = self.score(sentences)\n",
        "    high_score_index = np.argmax(scores)\n",
        "\n",
        "    return sentences[high_score_index]\n",
        "\n",
        "  # Score each sentence based on some criteria\n",
        "  def score(self, sentences):\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "      score = 0\n",
        "      score += self.length_score(sentence)\n",
        "      # add other criteria for scoring\n",
        "      # ...\n",
        "      # ...\n",
        "      scores.append(score)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "  # For beginners, we want to favor shorter sentences\n",
        "  # This method should change as we increase difficulty level\n",
        "  def length_score(self, sentence):\n",
        "    length = len(sentence)\n",
        "\n",
        "    if self.level == 'beginner':\n",
        "      if length > 0 and length <= 15:\n",
        "        return 6\n",
        "      elif length > 15 and length <= 25:\n",
        "        return 10\n",
        "      elif length > 25 and length <= 35:\n",
        "        return 7\n",
        "      elif length > 35 and length <= 45:\n",
        "        return 3\n",
        "      elif length > 45 and length <= 55:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "    else:\n",
        "      raise('support for non-beginners is not supported')\n",
        "\n",
        "  # For beginners, easier the better!\n",
        "  # This method should change as we increase difficulty level\n",
        "  def word_difficulty(self, sentence):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVxCTzArHKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGenerator():\n",
        "  def __init__(self, language_model=None, evaluator=None):\n",
        "    if language_model == None:\n",
        "      self.language_model = LanguageModel()\n",
        "    else:\n",
        "      self.language_model = language_model\n",
        "    if evaluator == None:\n",
        "      self.evaluator = SentenceEvaluator()\n",
        "    else:\n",
        "      self.evaluator = evaluator\n",
        "\n",
        "  # Generate a sentence, pick the best one based on evaluation, return the sentence\n",
        "  def generate(self, print_all_sentences=False):\n",
        "    # Determine the prefix/suffix based on some kind of criteria that is learned over time\n",
        "    prefix = self.determine_prefix()\n",
        "    if prefix == '<|startoftext|>':\n",
        "      include_prefix = False\n",
        "    else:\n",
        "      include_prefix = True\n",
        "    suffix = self.determine_suffix()\n",
        "\n",
        "    sentences = self.language_model.generate_text(prefix=prefix, suffix=suffix, include_prefix=include_prefix, nsamples=10)\n",
        "    ### TODO: May be good to remove newlines from sentences here ###\n",
        "    best_sentence = self.evaluator.evaluate(sentences)\n",
        "\n",
        "    if print_all_sentences:\n",
        "      for sentence in sentences:\n",
        "        print(sentence)\n",
        "\n",
        "    return best_sentence\n",
        "\n",
        "  def determine_prefix(self):\n",
        "    # Good simple sentence starters...\n",
        "    starters = ['I', 'You', 'The', 'It', '<|startoftext|>', 'This', 'My', 'What', 'When', 'Then', 'Why', 'Who', 'Where']\n",
        "    random_index = np.random.randint(0, len(starters)) \n",
        "\n",
        "    return starters[random_index]\n",
        "\n",
        "  def determine_suffix(self):\n",
        "    return '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpZvu9GMVu-O",
        "colab_type": "text"
      },
      "source": [
        "## Tutoring System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZz5lJCgeoDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB-19P2Rlh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42d6613c-5b30-44bc-c3a9-c60b040a3adf"
      },
      "source": [
        "model = LanguageModel('124M', train_steps=100)\n",
        "generator = SentenceGenerator(language_model=model)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching checkpoint: 1.05Mit [00:00, 272Mit/s]                                                      \u001b[A\n",
            "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\u001b[A\n",
            "Fetching encoder.json: 1.05Mit [00:00, 93.0Mit/s]                                                   \u001b[A\n",
            "Fetching hparams.json:   0%|                                            | 0.00/90.0 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching hparams.json: 1.05Mit [00:00, 530Mit/s]                                                    \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading {model_name} model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   1%|▏                | 5.24M/498M [00:00<00:11, 43.0Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   8%|█▎               | 38.8M/498M [00:00<00:07, 58.2Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  10%|█▊               | 51.4M/498M [00:00<00:06, 69.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  15%|██▌              | 76.5M/498M [00:00<00:04, 86.1Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  20%|███▌              | 98.6M/498M [00:00<00:03, 105Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  23%|████▍              | 115M/498M [00:00<00:03, 103Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  28%|█████▍             | 142M/498M [00:00<00:02, 125Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  32%|██████             | 159M/498M [00:00<00:02, 119Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  37%|██████▉            | 182M/498M [00:01<00:02, 139Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  41%|███████▊           | 203M/498M [00:01<00:01, 154Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  46%|████████▋          | 228M/498M [00:01<00:01, 155Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  49%|█████████▎         | 245M/498M [00:01<00:01, 138Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  54%|██████████▎        | 269M/498M [00:01<00:01, 144Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  59%|███████████▎       | 296M/498M [00:01<00:01, 166Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  63%|████████████       | 315M/498M [00:01<00:01, 157Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  70%|█████████████▏     | 347M/498M [00:02<00:00, 185Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  74%|██████████████     | 369M/498M [00:02<00:00, 186Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  78%|██████████████▉    | 390M/498M [00:02<00:00, 171Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  82%|███████████████▋   | 410M/498M [00:02<00:00, 132Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  88%|████████████████▋  | 437M/498M [00:02<00:00, 152Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  92%|█████████████████▍ | 456M/498M [00:02<00:00, 117Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  95%|██████████████████ | 473M/498M [00:02<00:00, 124Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  98%|█████████████████▋| 489M/498M [00:03<00:00, 70.6Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 138Mit/s]                                   \u001b[A\n",
            "Fetching model.ckpt.index:   0%|                                       | 0.00/5.21k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 394Mit/s]                                                \u001b[A\n",
            "Fetching model.ckpt.meta:   0%|                                         | 0.00/471k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 140Mit/s]                                                 \u001b[A\n",
            "Fetching vocab.bpe:   0%|                                               | 0.00/456k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 177Mit/s]                                                       \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:28<00:00, 28.04s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 6312984 tokens\n",
            "Training...\n",
            "[1 | 7.80] loss=3.59 avg=3.59\n",
            "[2 | 9.99] loss=3.73 avg=3.66\n",
            "[3 | 12.19] loss=3.63 avg=3.65\n",
            "[4 | 14.39] loss=3.51 avg=3.61\n",
            "[5 | 16.60] loss=3.29 avg=3.55\n",
            "[6 | 18.81] loss=3.46 avg=3.53\n",
            "[7 | 21.03] loss=3.43 avg=3.52\n",
            "[8 | 23.27] loss=3.38 avg=3.50\n",
            "[9 | 25.51] loss=3.37 avg=3.48\n",
            "[10 | 27.74] loss=3.40 avg=3.47\n",
            "[11 | 29.99] loss=3.29 avg=3.46\n",
            "[12 | 32.24] loss=3.21 avg=3.44\n",
            "[13 | 34.49] loss=3.33 avg=3.43\n",
            "[14 | 36.74] loss=2.92 avg=3.39\n",
            "[15 | 39.02] loss=3.46 avg=3.39\n",
            "[16 | 41.28] loss=3.38 avg=3.39\n",
            "[17 | 43.55] loss=2.94 avg=3.36\n",
            "[18 | 45.80] loss=3.28 avg=3.36\n",
            "[19 | 48.08] loss=3.38 avg=3.36\n",
            "[20 | 50.34] loss=3.15 avg=3.35\n",
            "[21 | 52.62] loss=3.36 avg=3.35\n",
            "[22 | 54.89] loss=3.20 avg=3.34\n",
            "[23 | 57.17] loss=3.41 avg=3.35\n",
            "[24 | 59.46] loss=3.32 avg=3.34\n",
            "[25 | 61.76] loss=2.89 avg=3.32\n",
            "[26 | 64.06] loss=3.12 avg=3.32\n",
            "[27 | 66.36] loss=3.23 avg=3.31\n",
            "[28 | 68.67] loss=3.09 avg=3.30\n",
            "[29 | 70.98] loss=3.25 avg=3.30\n",
            "[30 | 73.30] loss=3.11 avg=3.29\n",
            "[31 | 75.63] loss=3.29 avg=3.29\n",
            "[32 | 77.95] loss=3.34 avg=3.29\n",
            "[33 | 80.28] loss=3.52 avg=3.30\n",
            "[34 | 82.62] loss=3.40 avg=3.31\n",
            "[35 | 84.96] loss=3.13 avg=3.30\n",
            "[36 | 87.32] loss=3.02 avg=3.29\n",
            "[37 | 89.67] loss=3.51 avg=3.30\n",
            "[38 | 92.02] loss=3.37 avg=3.30\n",
            "[39 | 94.40] loss=3.31 avg=3.30\n",
            "[40 | 96.77] loss=3.21 avg=3.30\n",
            "[41 | 99.14] loss=3.37 avg=3.30\n",
            "[42 | 101.52] loss=3.20 avg=3.30\n",
            "[43 | 103.91] loss=3.23 avg=3.30\n",
            "[44 | 106.29] loss=3.18 avg=3.29\n",
            "[45 | 108.69] loss=3.22 avg=3.29\n",
            "[46 | 111.11] loss=3.03 avg=3.28\n",
            "[47 | 113.54] loss=3.29 avg=3.28\n",
            "[48 | 115.95] loss=3.28 avg=3.28\n",
            "[49 | 118.38] loss=3.17 avg=3.28\n",
            "[50 | 120.82] loss=3.33 avg=3.28\n",
            "[51 | 123.28] loss=3.32 avg=3.28\n",
            "[52 | 125.75] loss=3.25 avg=3.28\n",
            "[53 | 128.22] loss=3.21 avg=3.28\n",
            "[54 | 130.74] loss=3.34 avg=3.28\n",
            "[55 | 133.25] loss=3.56 avg=3.29\n",
            "[56 | 135.78] loss=3.25 avg=3.29\n",
            "[57 | 138.33] loss=3.28 avg=3.29\n",
            "[58 | 140.89] loss=3.20 avg=3.29\n",
            "[59 | 143.44] loss=2.76 avg=3.27\n",
            "[60 | 145.99] loss=3.10 avg=3.27\n",
            "[61 | 148.54] loss=3.14 avg=3.27\n",
            "[62 | 151.09] loss=3.27 avg=3.27\n",
            "[63 | 153.64] loss=3.33 avg=3.27\n",
            "[64 | 156.20] loss=3.37 avg=3.27\n",
            "[65 | 158.75] loss=3.43 avg=3.27\n",
            "[66 | 161.28] loss=3.43 avg=3.28\n",
            "[67 | 163.81] loss=3.31 avg=3.28\n",
            "[68 | 166.34] loss=3.29 avg=3.28\n",
            "[69 | 168.85] loss=3.19 avg=3.28\n",
            "[70 | 171.34] loss=3.12 avg=3.27\n",
            "[71 | 173.83] loss=3.15 avg=3.27\n",
            "[72 | 176.32] loss=3.32 avg=3.27\n",
            "[73 | 178.80] loss=3.30 avg=3.27\n",
            "[74 | 181.28] loss=2.96 avg=3.27\n",
            "[75 | 183.75] loss=3.10 avg=3.26\n",
            "[76 | 186.24] loss=3.19 avg=3.26\n",
            "[77 | 188.71] loss=3.45 avg=3.27\n",
            "[78 | 191.19] loss=3.16 avg=3.26\n",
            "[79 | 193.66] loss=3.02 avg=3.26\n",
            "[80 | 196.13] loss=3.13 avg=3.26\n",
            "[81 | 198.61] loss=2.93 avg=3.25\n",
            "[82 | 201.07] loss=3.47 avg=3.25\n",
            "[83 | 203.55] loss=3.34 avg=3.26\n",
            "[84 | 206.03] loss=3.09 avg=3.25\n",
            "[85 | 208.52] loss=3.14 avg=3.25\n",
            "[86 | 211.01] loss=3.05 avg=3.25\n",
            "[87 | 213.50] loss=3.09 avg=3.25\n",
            "[88 | 216.03] loss=3.13 avg=3.24\n",
            "[89 | 218.55] loss=3.25 avg=3.24\n",
            "[90 | 221.06] loss=2.92 avg=3.24\n",
            "[91 | 223.58] loss=3.46 avg=3.24\n",
            "[92 | 226.10] loss=3.31 avg=3.24\n",
            "[93 | 228.63] loss=3.06 avg=3.24\n",
            "[94 | 231.18] loss=3.39 avg=3.24\n",
            "[95 | 233.70] loss=3.26 avg=3.24\n",
            "[96 | 236.25] loss=3.35 avg=3.24\n",
            "[97 | 238.78] loss=2.76 avg=3.24\n",
            "[98 | 241.29] loss=3.04 avg=3.23\n",
            "[99 | 243.82] loss=3.26 avg=3.23\n",
            "[100 | 246.34] loss=2.99 avg=3.23\n",
            "Saving checkpoint/run1/model-100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIEVzlnWc9EF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "9ed915fd-776c-4c0a-b398-08015dda36b9"
      },
      "source": [
        "best_sentence = generator.generate(print_all_sentences=True)\n",
        "print(\"Best Sentence: \", best_sentence)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You had my cat-sized tummy , ' and the only thing that it was really worth it was the cat-sized tummy , and I told her to put it back \n",
            "You is not going to run any further until I am satisfied with your home \n",
            "You get in a cow carriage and get a cow out of the barn and drive on into the country\n",
            "You have made an acquaintance with the man you are visiting , and he has a son \n",
            "You have given it me only fifteen minutes to go and we shall be all right , but the sun is just coming over the hill and there is , I should say , no sign of it yet \n",
            "You have a new car and a new wife and a new baby<|endoftext|>Here is my father 's letter from England , which I will soon give to your mother 's , and give you some encouragement , and you will have a good time , and I shall be able to see you again , and I 'll do no wrong , and do not feel any fear lest you do wrong , and I will not be ashamed of you , and you shall always have my best wishes , which are always the best \n",
            "You : ''\n",
            "`` I can neither speak nor read , '' said Mr\n",
            "You will see later\n",
            "You think you can do it ?\n",
            "I hope you can ?\n",
            "I do not know if it is possible to do it , but I hope you can try , if you have not yet \n",
            "You think you can make me a little doll , ''\n",
            "asked Diana , who was very busy , and the child was playing with the doll \n",
            "Best Sentence:  You will see later\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}