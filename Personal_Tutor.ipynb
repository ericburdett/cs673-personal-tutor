{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personal-Tutor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqQfdDyiqQX60KPzdqfMFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs673-personal-tutor/blob/master/Personal_Tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngPwFJK2Q3is",
        "colab_type": "text"
      },
      "source": [
        "# Personal Tutor\n",
        "\n",
        "This notebook contains code for the Personal Tutor System built for CS673: Computational Creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzpCKzikWxA3",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUN5YCJsWzAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "import pandas as pd\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_JZMaHVEOg",
        "colab_type": "text"
      },
      "source": [
        "## Word Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx7LXT0sVLjI",
        "colab_type": "code",
        "outputId": "3586a358-6449-4af2-df95-99e06786c1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Download the simple word distribution from GitHub\n",
        "!wget -O word_dist_full.csv https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 16:28:36--  https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163042 (159K) [text/plain]\n",
            "Saving to: ‘word_dist_full.csv’\n",
            "\n",
            "word_dist_full.csv  100%[===================>] 159.22K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-04 16:28:37 (4.69 MB/s) - ‘word_dist_full.csv’ saved [163042/163042]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWVYaEVrV065",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordDist(Dataset):\n",
        "  def __init__(self):\n",
        "    self.df = pd.read_csv('word_dist_full.csv', header=None, names=['word', 'freq'])\n",
        "  \n",
        "  def getdf(self):\n",
        "    return self.df\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.df['word'][index], self.df['freq'][index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP2HehFTXIIN",
        "colab_type": "code",
        "outputId": "79f15e7c-b850-4dd1-b62b-c25d6c825301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "words = WordDist()\n",
        "print('Num Words: ', words)\n",
        "words[0:20]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Words:  <__main__.WordDist object at 0x7f04d3c68908>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0      the\n",
              " 1       of\n",
              " 2      and\n",
              " 3       to\n",
              " 4        a\n",
              " 5       in\n",
              " 6      for\n",
              " 7       is\n",
              " 8       on\n",
              " 9     that\n",
              " 10      by\n",
              " 11    this\n",
              " 12    with\n",
              " 13       i\n",
              " 14     you\n",
              " 15      it\n",
              " 16     not\n",
              " 17      or\n",
              " 18      be\n",
              " 19     are\n",
              " Name: word, dtype: object, 0     23135851162\n",
              " 1     13151942776\n",
              " 2     12997637966\n",
              " 3     12136980858\n",
              " 4      9081174698\n",
              " 5      8469404971\n",
              " 6      5933321709\n",
              " 7      4705743816\n",
              " 8      3750423199\n",
              " 9      3400031103\n",
              " 10     3350048871\n",
              " 11     3228469771\n",
              " 12     3183110675\n",
              " 13     3086225277\n",
              " 14     2996181025\n",
              " 15     2813163874\n",
              " 16     2633487141\n",
              " 17     2590739907\n",
              " 18     2398724162\n",
              " 19     2393614870\n",
              " Name: freq, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9LYBu6EVIkA",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dby_PgXUMtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a few different corpuses to work with GPT2\n",
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "!tar -xvf text_files.tar.gz\n",
        "!rm text_files.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s92Y9GsgMe5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "70146700-e494-4f7d-b78f-dfa7d92a62cf"
      },
      "source": [
        "# Download the children's book corpus from GitHub\n",
        "!wget -O cbt.txt https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/cbt_train.txt"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 18:41:03--  https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/cbt_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25742364 (25M) [text/plain]\n",
            "Saving to: ‘cbt.txt’\n",
            "\n",
            "cbt.txt             100%[===================>]  24.55M  41.1MB/s    in 0.6s    \n",
            "\n",
            "2020-02-04 18:41:04 (41.1 MB/s) - ‘cbt.txt’ saved [25742364/25742364]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWjuDBUNKE8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0b86c2b9-8b3b-4881-a18f-77ef38309f19"
      },
      "source": [
        "!pip install gpt-2-simple"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD6Mef5tL_oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1zeZwTfg4jH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d4188ca-3f6b-4691-ed0a-687c4c01a3f6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cbt.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x3VQUengj5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3GXmNGpMDAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2dacec0-de63-44cf-bfab-9df3e55e4299"
      },
      "source": [
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "  print(\"Downloading {model_name} model...\")\n",
        "  gpt2.download_gpt2(model_name=model_name)\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              'cbt.txt',\n",
        "              model_name=model_name,\n",
        "              steps=200)   # steps is max number of training steps"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching checkpoint: 1.05Mit [00:00, 84.8Mit/s]                                                     \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading {model_name} model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\u001b[A\n",
            "Fetching encoder.json: 1.05Mit [00:00, 70.3Mit/s]                                                   \u001b[A\n",
            "Fetching hparams.json:   0%|                                            | 0.00/90.0 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching hparams.json: 1.05Mit [00:00, 250Mit/s]                                                    \u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   1%|▎                | 7.34M/498M [00:00<00:06, 70.7Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   2%|▎                | 10.5M/498M [00:00<00:09, 49.9Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   3%|▌                | 14.7M/498M [00:00<00:13, 36.7Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   6%|▉                | 28.3M/498M [00:00<00:10, 45.7Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   7%|█▏               | 34.6M/498M [00:00<00:09, 48.7Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:   9%|█▌               | 45.1M/498M [00:00<00:08, 56.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  11%|█▊               | 52.4M/498M [00:00<00:07, 60.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  13%|██▏              | 65.0M/498M [00:00<00:06, 71.5Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  15%|██▌              | 76.5M/498M [00:01<00:05, 76.1Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  17%|██▉              | 87.0M/498M [00:01<00:04, 82.2Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  19%|███▎             | 96.5M/498M [00:01<00:05, 80.0Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  21%|███▊              | 107M/498M [00:01<00:04, 85.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  24%|████▎             | 120M/498M [00:01<00:04, 94.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  26%|████▋             | 130M/498M [00:01<00:04, 88.3Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  30%|█████▋             | 149M/498M [00:01<00:03, 104Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  34%|██████▌            | 171M/498M [00:01<00:02, 123Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  39%|███████▍           | 194M/498M [00:01<00:02, 143Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  43%|████████           | 212M/498M [00:02<00:01, 147Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  47%|████████▉          | 233M/498M [00:02<00:01, 161Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  51%|█████████▌         | 252M/498M [00:02<00:01, 151Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  55%|██████████▎        | 272M/498M [00:02<00:01, 161Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  59%|███████████▏       | 293M/498M [00:02<00:01, 173Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  63%|███████████▉       | 312M/498M [00:02<00:01, 178Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  67%|████████████▋      | 331M/498M [00:02<00:01, 154Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  70%|█████████████▎     | 348M/498M [00:02<00:01, 133Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  73%|█████████████▊     | 363M/498M [00:03<00:01, 120Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  76%|██████████████▎    | 376M/498M [00:03<00:01, 114Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  79%|███████████████    | 394M/498M [00:03<00:00, 128Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  82%|██████████████▊   | 409M/498M [00:03<00:00, 97.4Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  85%|███████████████▏  | 422M/498M [00:03<00:00, 80.1Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  87%|███████████████▌  | 432M/498M [00:03<00:00, 83.2Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  89%|████████████████  | 442M/498M [00:04<00:00, 87.2Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  91%|████████████████▍ | 453M/498M [00:04<00:00, 81.6Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  93%|████████████████▋ | 462M/498M [00:04<00:00, 69.4Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  95%|█████████████████ | 471M/498M [00:04<00:00, 57.6Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  96%|█████████████████▎| 478M/498M [00:04<00:00, 58.6Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  98%|█████████████████▌| 485M/498M [00:04<00:00, 44.4Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001:  99%|█████████████████▊| 492M/498M [00:05<00:00, 45.9Mit/s]\u001b[A\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 97.9Mit/s]                                  \u001b[A\n",
            "Fetching model.ckpt.index:   0%|                                       | 0.00/5.21k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 329Mit/s]                                                \u001b[A\n",
            "Fetching model.ckpt.meta:   0%|                                         | 0.00/471k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 94.3Mit/s]                                                \u001b[A\n",
            "Fetching vocab.bpe:   0%|                                               | 0.00/456k [00:00<?, ?it/s]\u001b[A\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 113Mit/s]                                                       \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:31<00:00, 31.72s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 6312984 tokens\n",
            "Training...\n",
            "[1 | 7.30] loss=3.77 avg=3.77\n",
            "[2 | 8.59] loss=3.21 avg=3.49\n",
            "[3 | 9.89] loss=3.55 avg=3.51\n",
            "[4 | 11.16] loss=3.23 avg=3.44\n",
            "[5 | 12.44] loss=3.27 avg=3.40\n",
            "[6 | 13.75] loss=3.60 avg=3.44\n",
            "[7 | 15.03] loss=3.38 avg=3.43\n",
            "[8 | 16.36] loss=3.57 avg=3.45\n",
            "[9 | 17.66] loss=3.73 avg=3.48\n",
            "[10 | 18.93] loss=3.31 avg=3.46\n",
            "[11 | 20.21] loss=3.33 avg=3.45\n",
            "[12 | 21.49] loss=3.49 avg=3.45\n",
            "[13 | 22.77] loss=3.61 avg=3.47\n",
            "[14 | 24.04] loss=3.37 avg=3.46\n",
            "[15 | 25.33] loss=3.37 avg=3.45\n",
            "[16 | 26.60] loss=3.39 avg=3.45\n",
            "[17 | 27.88] loss=3.43 avg=3.45\n",
            "[18 | 29.18] loss=3.33 avg=3.44\n",
            "[19 | 30.45] loss=3.39 avg=3.44\n",
            "[20 | 31.73] loss=3.32 avg=3.43\n",
            "[21 | 33.01] loss=3.15 avg=3.42\n",
            "[22 | 34.28] loss=3.51 avg=3.42\n",
            "[23 | 35.57] loss=3.34 avg=3.42\n",
            "[24 | 36.84] loss=3.46 avg=3.42\n",
            "[25 | 38.13] loss=3.15 avg=3.41\n",
            "[26 | 39.40] loss=3.40 avg=3.41\n",
            "[27 | 40.69] loss=3.36 avg=3.40\n",
            "[28 | 41.97] loss=3.50 avg=3.41\n",
            "[29 | 43.25] loss=3.38 avg=3.41\n",
            "[30 | 44.54] loss=3.29 avg=3.40\n",
            "[31 | 45.83] loss=3.50 avg=3.41\n",
            "[32 | 47.11] loss=3.20 avg=3.40\n",
            "[33 | 48.41] loss=3.19 avg=3.39\n",
            "[34 | 49.70] loss=3.21 avg=3.39\n",
            "[35 | 50.97] loss=3.61 avg=3.39\n",
            "[36 | 52.26] loss=3.29 avg=3.39\n",
            "[37 | 53.57] loss=3.05 avg=3.38\n",
            "[38 | 54.85] loss=3.09 avg=3.37\n",
            "[39 | 56.13] loss=3.53 avg=3.37\n",
            "[40 | 57.41] loss=3.12 avg=3.37\n",
            "[41 | 58.67] loss=3.17 avg=3.36\n",
            "[42 | 59.95] loss=3.31 avg=3.36\n",
            "[43 | 61.22] loss=3.18 avg=3.35\n",
            "[44 | 62.50] loss=3.28 avg=3.35\n",
            "[45 | 63.78] loss=3.31 avg=3.35\n",
            "[46 | 65.05] loss=3.39 avg=3.35\n",
            "[47 | 66.32] loss=3.26 avg=3.35\n",
            "[48 | 67.61] loss=3.27 avg=3.35\n",
            "[49 | 68.90] loss=3.23 avg=3.34\n",
            "[50 | 70.19] loss=3.09 avg=3.34\n",
            "[51 | 71.47] loss=3.31 avg=3.34\n",
            "[52 | 72.74] loss=3.48 avg=3.34\n",
            "[53 | 74.03] loss=3.14 avg=3.34\n",
            "[54 | 75.31] loss=3.41 avg=3.34\n",
            "[55 | 76.59] loss=3.17 avg=3.33\n",
            "[56 | 77.86] loss=3.01 avg=3.33\n",
            "[57 | 79.15] loss=3.31 avg=3.33\n",
            "[58 | 80.44] loss=2.99 avg=3.32\n",
            "[59 | 81.73] loss=3.20 avg=3.32\n",
            "[60 | 83.00] loss=3.50 avg=3.32\n",
            "[61 | 84.29] loss=3.20 avg=3.32\n",
            "[62 | 85.59] loss=3.29 avg=3.32\n",
            "[63 | 86.89] loss=3.37 avg=3.32\n",
            "[64 | 88.17] loss=3.42 avg=3.32\n",
            "[65 | 89.47] loss=3.22 avg=3.32\n",
            "[66 | 90.75] loss=2.99 avg=3.31\n",
            "[67 | 92.02] loss=3.44 avg=3.31\n",
            "[68 | 93.29] loss=3.19 avg=3.31\n",
            "[69 | 94.57] loss=3.41 avg=3.31\n",
            "[70 | 95.87] loss=3.20 avg=3.31\n",
            "[71 | 97.15] loss=2.97 avg=3.30\n",
            "[72 | 98.44] loss=3.33 avg=3.30\n",
            "[73 | 99.72] loss=3.17 avg=3.30\n",
            "[74 | 100.99] loss=3.09 avg=3.30\n",
            "[75 | 102.26] loss=3.21 avg=3.30\n",
            "[76 | 103.54] loss=3.09 avg=3.29\n",
            "[77 | 104.83] loss=3.08 avg=3.29\n",
            "[78 | 106.10] loss=3.18 avg=3.29\n",
            "[79 | 107.38] loss=3.56 avg=3.29\n",
            "[80 | 108.65] loss=3.18 avg=3.29\n",
            "[81 | 109.93] loss=3.02 avg=3.28\n",
            "[82 | 111.22] loss=3.21 avg=3.28\n",
            "[83 | 112.51] loss=3.11 avg=3.28\n",
            "[84 | 113.79] loss=3.38 avg=3.28\n",
            "[85 | 115.07] loss=3.08 avg=3.28\n",
            "[86 | 116.37] loss=3.25 avg=3.28\n",
            "[87 | 117.65] loss=3.20 avg=3.28\n",
            "[88 | 118.94] loss=3.32 avg=3.28\n",
            "[89 | 120.23] loss=3.30 avg=3.28\n",
            "[90 | 121.51] loss=3.29 avg=3.28\n",
            "[91 | 122.78] loss=3.19 avg=3.28\n",
            "[92 | 124.07] loss=3.16 avg=3.27\n",
            "[93 | 125.34] loss=2.88 avg=3.27\n",
            "[94 | 126.65] loss=3.04 avg=3.26\n",
            "[95 | 127.93] loss=3.18 avg=3.26\n",
            "[96 | 129.22] loss=3.19 avg=3.26\n",
            "[97 | 130.49] loss=3.36 avg=3.26\n",
            "[98 | 131.77] loss=3.02 avg=3.26\n",
            "[99 | 133.04] loss=3.02 avg=3.26\n",
            "[100 | 134.33] loss=3.24 avg=3.26\n",
            "======== SAMPLE 1 ========\n",
            " selfish .\n",
            "The doctor 's face is flushed and stern .\n",
            "` I have not a tooth ! '\n",
            "I say .\n",
            "` The Doctor is a doctor . '\n",
            "` Oh !\n",
            "my boy , I 've known a doctor . '\n",
            "I say he will tell us our last name , and that name , and my little boy .\n",
            "You shall see , I 'm going to be glad . '\n",
            "` Do me a justice , '' said the doctor .\n",
            "I 'll tell you something , '' said I , and was off to run my hands in despair ; but the doctor had left me too much slack on account of his reluctance .\n",
            "I did him another justice , he said ; ` You are your best little boy .\n",
            "He 's as good as a girl and he 'c n't a great boy yet . '\n",
            "I must take care not to see you in any way , though your father would think I was so little , my boy ; and I was not at all content in that , though there was quite the difference between us . '\n",
            "` Is , '' said the doctor , `` I 've to be at my father 's and mother 's door ? '\n",
            "I suppose it 's time for us to go over there and make up our minds .\n",
            "You come home alone .\n",
            "Your husband must come over , and I 'll go see ' us ,'' said I .\n",
            "` I 'm in my daughter 's room , '' he said , as quickly as possible .\n",
            "`` I 'm a very good kid and I know it .\n",
            "But I 'm just a kid .\n",
            "You know I do n't mind . '\n",
            "` My child ? '\n",
            "I do n't mind .\n",
            "But I 'm trying to teach you something . ' '\n",
            "My child , he said , ` I 've told you before , and I do n't think you 'd learn to care .\n",
            "But you 'll learn .\n",
            "It 's my first time ever teaching a boy , and I can teach a boy , too . '\n",
            "` No ? '\n",
            "said the doctor .\n",
            "` I 've taught you -- taught me before .\n",
            "You are a poor little child , and you ought to learn . '\n",
            "` Well , he knows , '' said I , `` but I ca n't -- I ca n't be ashamed of being a boy .\n",
            "The baby is quite a little . '\n",
            "The doctor thought over a moment . ' '\n",
            "I n't the little one to talk about that , '' said I , `` I mean my father 's own .\n",
            "You and I do not talk about it because we are going to be a little bit of a mother-in-law .\n",
            "I ca n't say you 'd be ashamed in this . '\n",
            "` But I thought you could teach me , Doctor ?\n",
            "I know the baby is only a baby .\n",
            "` No . '\n",
            "I mean there 'll be no talk . '\n",
            "` I can teach you , Doctor , then I 'd give you -- give you -- give you -- give you . '\n",
            "` But I thought as I talked -- I thought I taught you something . '\n",
            "` I taught me to care when you brought me up in the world and tried to make you and me go out of and be the things you could be .\n",
            "But you knew you had to be cared for .\n",
            "You could teach your baby -- teach him to care for your own children and be the children you wanted .\n",
            "But you had to be cared for , and I 'd have done that to you and your infant father and your daughter .\n",
            "And I never thought that I could teach you something . '\n",
            "And the doctor thought it through for a lifetime of learning , a lifetime of learning , and he was proud as she was .\n",
            "But the doctor 's heart was in him .\n",
            "He was sad .\n",
            "He would have no time to tell of it .\n",
            "But he listened .\n",
            "He wanted to go over and make up his mind .\n",
            "His heart felt good , and it was n't as if there was something 's wrong with him .\n",
            "He had only a week away .\n",
            "He wanted to go over and think it over .\n",
            "He wanted to know what had happened .\n",
            "He wanted to cry and grow up .\n",
            "So he had only three weeks to go .\n",
            "You know , it 's never a good idea to cry too hard .\n",
            "I said no more .\n",
            "You should be able to take a wee little over to his room ; and then you n't go down into his room for weeks if you do 're too afraid of crying .\n",
            "He needed to be given the opportunity of knowing what had happened .\n",
            "But he wanted to go over to the doctor who taught him what had happened .\n",
            "He said all the time , ' You 're going to come over and explain .\n",
            "It 's impossible . '\n",
            "I do n't\n",
            "\n",
            "[101 | 150.10] loss=3.20 avg=3.25\n",
            "[102 | 151.37] loss=3.10 avg=3.25\n",
            "[103 | 152.65] loss=3.02 avg=3.25\n",
            "[104 | 153.92] loss=3.37 avg=3.25\n",
            "[105 | 155.19] loss=3.51 avg=3.25\n",
            "[106 | 156.46] loss=3.11 avg=3.25\n",
            "[107 | 157.74] loss=3.25 avg=3.25\n",
            "[108 | 159.03] loss=3.41 avg=3.25\n",
            "[109 | 160.30] loss=3.16 avg=3.25\n",
            "[110 | 161.56] loss=3.08 avg=3.25\n",
            "[111 | 162.86] loss=3.05 avg=3.25\n",
            "[112 | 164.14] loss=3.21 avg=3.25\n",
            "[113 | 165.41] loss=3.20 avg=3.25\n",
            "[114 | 166.68] loss=3.13 avg=3.24\n",
            "[115 | 167.95] loss=3.10 avg=3.24\n",
            "[116 | 169.24] loss=3.38 avg=3.24\n",
            "[117 | 170.51] loss=3.12 avg=3.24\n",
            "[118 | 171.78] loss=3.29 avg=3.24\n",
            "[119 | 173.05] loss=3.49 avg=3.25\n",
            "[120 | 174.34] loss=3.29 avg=3.25\n",
            "[121 | 175.63] loss=3.27 avg=3.25\n",
            "[122 | 176.90] loss=2.97 avg=3.24\n",
            "[123 | 178.17] loss=3.09 avg=3.24\n",
            "[124 | 179.44] loss=3.41 avg=3.24\n",
            "[125 | 180.72] loss=3.00 avg=3.24\n",
            "[126 | 182.00] loss=3.07 avg=3.24\n",
            "[127 | 183.27] loss=3.07 avg=3.24\n",
            "[128 | 184.54] loss=3.11 avg=3.23\n",
            "[129 | 185.83] loss=2.85 avg=3.23\n",
            "[130 | 187.10] loss=3.30 avg=3.23\n",
            "[131 | 188.39] loss=3.22 avg=3.23\n",
            "[132 | 189.69] loss=3.57 avg=3.23\n",
            "[133 | 190.96] loss=3.13 avg=3.23\n",
            "[134 | 192.25] loss=3.20 avg=3.23\n",
            "[135 | 193.53] loss=3.61 avg=3.24\n",
            "[136 | 194.81] loss=3.03 avg=3.23\n",
            "[137 | 196.08] loss=3.16 avg=3.23\n",
            "[138 | 197.38] loss=3.01 avg=3.23\n",
            "[139 | 198.66] loss=2.96 avg=3.23\n",
            "[140 | 199.93] loss=3.36 avg=3.23\n",
            "[141 | 201.21] loss=3.21 avg=3.23\n",
            "[142 | 202.48] loss=3.00 avg=3.23\n",
            "[143 | 203.76] loss=3.33 avg=3.23\n",
            "[144 | 205.06] loss=3.20 avg=3.23\n",
            "[145 | 206.34] loss=3.23 avg=3.23\n",
            "[146 | 207.63] loss=3.43 avg=3.23\n",
            "[147 | 208.93] loss=3.05 avg=3.23\n",
            "[148 | 210.22] loss=3.30 avg=3.23\n",
            "[149 | 211.50] loss=3.01 avg=3.23\n",
            "[150 | 212.78] loss=3.24 avg=3.23\n",
            "[151 | 214.06] loss=3.21 avg=3.23\n",
            "[152 | 215.34] loss=3.17 avg=3.22\n",
            "[153 | 216.64] loss=2.79 avg=3.22\n",
            "[154 | 217.94] loss=3.04 avg=3.22\n",
            "[155 | 219.21] loss=3.30 avg=3.22\n",
            "[156 | 220.50] loss=2.97 avg=3.21\n",
            "[157 | 221.76] loss=3.23 avg=3.21\n",
            "[158 | 223.06] loss=3.21 avg=3.21\n",
            "[159 | 224.33] loss=3.14 avg=3.21\n",
            "[160 | 225.62] loss=3.28 avg=3.21\n",
            "[161 | 226.90] loss=3.10 avg=3.21\n",
            "[162 | 228.18] loss=3.13 avg=3.21\n",
            "[163 | 229.45] loss=3.38 avg=3.21\n",
            "[164 | 230.73] loss=3.16 avg=3.21\n",
            "[165 | 232.02] loss=3.03 avg=3.21\n",
            "[166 | 233.31] loss=3.21 avg=3.21\n",
            "[167 | 234.61] loss=3.21 avg=3.21\n",
            "[168 | 235.88] loss=3.30 avg=3.21\n",
            "[169 | 237.15] loss=3.32 avg=3.21\n",
            "[170 | 238.44] loss=3.24 avg=3.21\n",
            "[171 | 239.73] loss=3.08 avg=3.21\n",
            "[172 | 241.01] loss=2.98 avg=3.21\n",
            "[173 | 242.29] loss=2.78 avg=3.20\n",
            "[174 | 243.59] loss=3.20 avg=3.20\n",
            "[175 | 244.86] loss=2.87 avg=3.20\n",
            "[176 | 246.12] loss=2.95 avg=3.20\n",
            "[177 | 247.40] loss=2.92 avg=3.19\n",
            "[178 | 248.70] loss=3.08 avg=3.19\n",
            "[179 | 249.98] loss=3.07 avg=3.19\n",
            "[180 | 251.26] loss=3.32 avg=3.19\n",
            "[181 | 252.54] loss=3.13 avg=3.19\n",
            "[182 | 253.83] loss=3.15 avg=3.19\n",
            "[183 | 255.11] loss=3.06 avg=3.19\n",
            "[184 | 256.39] loss=3.19 avg=3.19\n",
            "[185 | 257.67] loss=3.29 avg=3.19\n",
            "[186 | 258.95] loss=3.12 avg=3.19\n",
            "[187 | 260.24] loss=3.32 avg=3.19\n",
            "[188 | 261.52] loss=3.23 avg=3.19\n",
            "[189 | 262.80] loss=2.88 avg=3.19\n",
            "[190 | 264.09] loss=3.20 avg=3.19\n",
            "[191 | 265.37] loss=3.16 avg=3.19\n",
            "[192 | 266.65] loss=3.05 avg=3.19\n",
            "[193 | 267.93] loss=2.94 avg=3.18\n",
            "[194 | 269.24] loss=3.13 avg=3.18\n",
            "[195 | 270.54] loss=3.17 avg=3.18\n",
            "[196 | 271.83] loss=3.20 avg=3.18\n",
            "[197 | 273.11] loss=3.18 avg=3.18\n",
            "[198 | 274.39] loss=2.86 avg=3.18\n",
            "[199 | 275.67] loss=3.03 avg=3.18\n",
            "[200 | 276.94] loss=3.48 avg=3.18\n",
            "Saving checkpoint/run1/model-200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpZvu9GMVu-O",
        "colab_type": "text"
      },
      "source": [
        "## Tutoring System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMTOVOKRWcIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def determine_prefix():\n",
        "  return '<|startoftext|>'\n",
        "\n",
        "def determine_suffix():\n",
        "  return '.'\n",
        "\n",
        "# What impacts the quality of a good sentence?\n",
        "def determine_best_sentence(sentences):\n",
        "  # Shorter sentences\n",
        "  min_index=0\n",
        "  for i in range(1, len(sentences)):\n",
        "    if len(sentences[i]) < len(sentences[min_index]):\n",
        "      min_index = i\n",
        "\n",
        "  # Contains lower word difficulty\n",
        "  # -- Utilizes word difficulty\n",
        "\n",
        "  return sentences[i]\n",
        "\n",
        "def generate_sentence():\n",
        "  prefix = determine_prefix()\n",
        "  suffix = determine_suffix()\n",
        "  sentences = gpt2.generate(sess, prefix=prefix, truncate=suffix, return_as_list=True, batch_size=5, nsamples=5, temperature=.9, include_prefix=False)\n",
        "  \n",
        "  return determine_best_sentence(sentences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB-19P2Rlh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09c52fdf-54e0-4234-9bd3-d45a10628aa2"
      },
      "source": [
        "generate_sentence()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Dead*tantahawk Nautilus Aardvarkly Thru Gromm --- ''\\nSo chirped And he felt himself to be lost in thought \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}