{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personal-Tutor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDM9llipKjODhuEvB7enZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs673-personal-tutor/blob/master/Personal_Tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngPwFJK2Q3is",
        "colab_type": "text"
      },
      "source": [
        "# Personal Tutor\n",
        "\n",
        "This notebook contains code for the Personal Tutor System built for CS673: Computational Creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzpCKzikWxA3",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn2-Iv5RGY_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "69f35801-3d90-4496-e8ae-1120f43a01e2"
      },
      "source": [
        "!pip install gpt-2-simple"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=daed177259285a509dfbba8d85d510cf781b554bf615f189fe21b37fd0d1b37f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUN5YCJsWzAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import string\n",
        "import gzip\n",
        "import tarfile\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iLmsxoZGjSB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "bc414120-e87f-4ec3-c57a-d8d5efab0932"
      },
      "source": [
        "# Download a few different corpuses to work with GPT2\n",
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "!tar -xvf text_files.tar.gz\n",
        "!rm text_files.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 17:33:49--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 34.237.183.157, 34.192.194.48, 34.231.49.99, ...\n",
            "Connecting to piazza.com (piazza.com)|34.237.183.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2020-02-06 17:33:49--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 54.230.18.161, 54.230.18.12, 54.230.18.115, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|54.230.18.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-02-06 17:33:50 (10.1 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "text_files/lotr.txt\n",
            "text_files/\n",
            "text_files/alma.txt\n",
            "text_files/abcd.txt\n",
            "text_files/tiny_shakespeare.txt\n",
            "text_files/abab.txt\n",
            "text_files/switch.bat.txt\n",
            "text_files/switch.sh\n",
            "text_files/test1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg7SoAGkGh3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5341c96b-2e13-4683-e6b4-76ce8400e5fa"
      },
      "source": [
        "# Download the children's book corpus from GitHub\n",
        "!wget -O cbt.txt https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/cbt_train.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 17:33:59--  https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/cbt_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25742364 (25M) [text/plain]\n",
            "Saving to: ‘cbt.txt’\n",
            "\n",
            "cbt.txt             100%[===================>]  24.55M  41.8MB/s    in 0.6s    \n",
            "\n",
            "2020-02-06 17:34:00 (41.8 MB/s) - ‘cbt.txt’ saved [25742364/25742364]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_JZMaHVEOg",
        "colab_type": "text"
      },
      "source": [
        "## Word Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx7LXT0sVLjI",
        "colab_type": "code",
        "outputId": "8ecc30ea-8454-4159-b3d0-e030a5674d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Download the simple word distribution from GitHub\n",
        "!wget -O word_dist_full.csv https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 17:34:16--  https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163042 (159K) [text/plain]\n",
            "Saving to: ‘word_dist_full.csv’\n",
            "\n",
            "\rword_dist_full.csv    0%[                    ]       0  --.-KB/s               \rword_dist_full.csv  100%[===================>] 159.22K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-06 17:34:16 (4.56 MB/s) - ‘word_dist_full.csv’ saved [163042/163042]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWVYaEVrV065",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordDist(Dataset):\n",
        "  def __init__(self):\n",
        "    self.df = pd.read_csv('word_dist_full.csv', header=None, names=['word', 'freq'])\n",
        "  \n",
        "  def getdf(self):\n",
        "    return self.df\n",
        "\n",
        "  def dict_normalized(self): \n",
        "    copy = self.df.copy()\n",
        "    copy['freq'] = copy['freq'] / copy['freq'].max()\n",
        "\n",
        "    return copy.set_index('word').to_dict()['freq']\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.df['word'][index], self.df['freq'][index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9LYBu6EVIkA",
        "colab_type": "text"
      },
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuQNroOuIg1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class that deals with training and generating text from GPT2\n",
        "class LanguageModel():\n",
        "  def __init__(self, model='124M', genre='children', train_steps=200, max_length=150):\n",
        "    self.download_model(model)\n",
        "    self.genre = genre\n",
        "    self.max_length = max_length\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    self.sess = gpt2.start_tf_sess()\n",
        "\n",
        "    if genre == 'children':\n",
        "      gpt2.finetune(self.sess, 'cbt.txt', model_name=model, steps=train_steps)\n",
        "    else:\n",
        "      raise('The specified genre does not exist')\n",
        "\n",
        "  # Returns a list of sample texts with a given prefix and suffix\n",
        "  def generate_text(self, prefix='<|startoftext|>', suffix='.', include_prefix=False, nsamples=5):\n",
        "    if nsamples < 1 or nsamples > 20:\n",
        "      raise('Error: nsamples must be within the range 1 <= x <= 20')\n",
        "\n",
        "    return gpt2.generate(self.sess, prefix=prefix, truncate=suffix, include_prefix=include_prefix, batch_size=nsamples, nsamples=nsamples, return_as_list=True, length=self.max_length)\n",
        "  \n",
        "  def download_model(self, model_name):\n",
        "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "      print(f\"Downloading {model_name} model...\")\n",
        "      gpt2.download_gpt2(model_name=model_name)\n",
        "    else:\n",
        "      print(f\"{model_name} model is already downloaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQxMeo2JNzSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A class that contains some knowledge that the user has acquired over time.\n",
        "# For example, it may hold the words that the user knows (and how well the user knows them)\n",
        "class UserKnowledge():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # We will likely need some place to store the knowledge we acquire about the user\n",
        "  # so that we can access it from session to session\n",
        "  def save_knowledge(self, path):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK1wDhdgJajQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class that evaluates sentences based on what the system knows about the user\n",
        "class SentenceEvaluator():\n",
        "  def __init__(self, level='beginner', user_knowledge=None):\n",
        "    self.level = 'beginner'\n",
        "    self.word_dist = WordDist().dict_normalized()\n",
        "    self.word_dist_threshold = 0.033\n",
        "    self.word_dist_threshold_step = 0.005\n",
        "    self.word_dist_difficulty_threshold = 7\n",
        "\n",
        "    if user_knowledge == None:\n",
        "      self.user_knowledge = UserKnowledge()\n",
        "    else:\n",
        "      self.user_knowledge = user_knowledge\n",
        "\n",
        "  # We will likely want some method to update the user_knowledge in the evaluator\n",
        "  # Maybe, we will only pass user_knowledge into the evaluate function?...\n",
        "  def update_user_knowledge(user_knowledge):\n",
        "    pass\n",
        "\n",
        "  # Score the sentences and return the sentence with the highest score\n",
        "  def evaluate(self, sentences):\n",
        "    scores = self.score(sentences)\n",
        "    high_score_index = np.argmax(scores)\n",
        "\n",
        "    return sentences[high_score_index]\n",
        "\n",
        "  # Score each sentence based on some criteria\n",
        "  def score(self, sentences):\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "      score = 0\n",
        "      score += self.length_score(sentence)\n",
        "      score += self.word_difficulty(sentence)\n",
        "\n",
        "      # add other criteria for scoring\n",
        "      # ...\n",
        "      # ...\n",
        "      scores.append(score)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "  # For beginners, we want to favor shorter sentences\n",
        "  # This method should change as we increase difficulty level\n",
        "  def length_score(self, sentence):\n",
        "    length = len(sentence)\n",
        "\n",
        "    if self.level == 'beginner':\n",
        "      if length > 0 and length <= 15:\n",
        "        return 6\n",
        "      elif length > 15 and length <= 25:\n",
        "        return 10\n",
        "      elif length > 25 and length <= 35:\n",
        "        return 7\n",
        "      elif length > 35 and length <= 45:\n",
        "        return 3\n",
        "      elif length > 45 and length <= 55:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "    else:\n",
        "      raise('support for non-beginners is not supported')\n",
        "\n",
        "  # For beginners, easier the better!\n",
        "  # This method should change as we increase difficulty level\n",
        "  def word_difficulty(self, sentence):\n",
        "    word_scores = []\n",
        "\n",
        "    for word in sentence.split(' '):\n",
        "      word = word.lower()\n",
        "      word_score = self.word_dist.get(word, 0) # Return the word or 0 if it doesn't exist\n",
        "\n",
        "      if word_score >= self.word_dist_threshold:\n",
        "        word_scores.append(10)\n",
        "      elif word_score >= self.word_dist_threshold - self.word_dist_threshold_step:\n",
        "        word_scores.append(8)\n",
        "      elif word_score >= self.word_dist_threshold - (2 * self.word_dist_threshold_step):\n",
        "        word_scores.append(6)\n",
        "      elif word_score >= self.word_dist_threshold - (3 * self.word_dist_threshold_step):\n",
        "        word_scores.append(4)\n",
        "      elif word_score >= self.word_dist_threshold - (4 * self.word_dist_threshold_step):\n",
        "        word_scores.append(2)\n",
        "      else:\n",
        "        word_scores.append(0)\n",
        "\n",
        "    score_med = np.median(word_scores)\n",
        "\n",
        "    if score_med >= self.word_dist_difficulty_threshold:\n",
        "      return 10\n",
        "    elif score_med >= self.word_dist_difficulty_threshold - 1:\n",
        "      return 8\n",
        "    elif score_med >= self.word_dist_difficulty_threshold - 2:\n",
        "      return 6\n",
        "    elif score_med >= self.word_dist_difficulty_threshold - 3:\n",
        "      return 4\n",
        "    elif score_med >= self.word_dist_difficulty_threshold - 4:\n",
        "      return 2\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVxCTzArHKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGenerator():\n",
        "  def __init__(self, language_model=None, evaluator=None):\n",
        "    if language_model == None:\n",
        "      self.language_model = LanguageModel()\n",
        "    else:\n",
        "      self.language_model = language_model\n",
        "    if evaluator == None:\n",
        "      self.evaluator = SentenceEvaluator()\n",
        "    else:\n",
        "      self.evaluator = evaluator\n",
        "\n",
        "  # Generate a sentence, pick the best one based on evaluation, return the sentence\n",
        "  def generate(self, print_all_sentences=False):\n",
        "    # Determine the prefix/suffix based on some kind of criteria that is learned over time\n",
        "    prefix = self.determine_prefix()\n",
        "    if prefix == '<|startoftext|>':\n",
        "      include_prefix = False\n",
        "    else:\n",
        "      include_prefix = True\n",
        "    suffix = self.determine_suffix()\n",
        "\n",
        "    sentences = self.language_model.generate_text(prefix=prefix, suffix=suffix, include_prefix=include_prefix, nsamples=10)\n",
        "    sentences = self.filter_punctuation(sentences)\n",
        "    best_sentence = self.evaluator.evaluate(sentences)\n",
        "\n",
        "    if print_all_sentences:\n",
        "      for sentence in sentences:\n",
        "        print(sentence)\n",
        "\n",
        "    return best_sentence\n",
        "  \n",
        "  # Used to filter unwanted punctuation GPT2 might produce, like newlines\n",
        "  def filter_punctuation(self, sentences):\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "      new_sentence = sentence.replace('\\n', ' ')\n",
        "      new_sentence = new_sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "      filtered_sentences.append(new_sentence)\n",
        "\n",
        "    return filtered_sentences\n",
        "\n",
        "  def determine_prefix(self):\n",
        "    # Good simple sentence starters...\n",
        "    starters = ['I', 'You', 'The', 'They', 'It', '<|startoftext|>', 'This', 'My', 'What', 'When', 'Then', 'Why', 'Who', 'Where']\n",
        "    random_index = np.random.randint(0, len(starters)) \n",
        "\n",
        "    return starters[random_index]\n",
        "\n",
        "  def determine_suffix(self):\n",
        "    return '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpZvu9GMVu-O",
        "colab_type": "text"
      },
      "source": [
        "## Tutoring System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfeiyHvE2AhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a1cd751d-f096-401f-c586-68925fd0833e"
      },
      "source": [
        "wd = WordDist().dict_normalized()\n",
        "wd['why']"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008298837620262524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB-19P2Rlh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LanguageModel('124M', train_steps=100) # Will fine-tune model everytime this is called! -- Will need to be fixed at some point\n",
        "generator = SentenceGenerator(language_model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIEVzlnWc9EF",
        "colab_type": "code",
        "outputId": "5747b1dc-c5a8-46ad-f737-398f825c05dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "generator = SentenceGenerator(language_model=model)\n",
        "best_sentence = generator.generate(print_all_sentences=True)\n",
        "print(\"Best Sentence: \", best_sentence)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "median:  4.0\n",
            "median:  3.0\n",
            "median:  4.0\n",
            "median:  10.0\n",
            "median:  8.0\n",
            "median:  10.0\n",
            "median:  10.0\n",
            "median:  4.0\n",
            "median:  0.0\n",
            "median:  10.0\n",
            "I told you she was a good girl and I had no real interest in her \n",
            "I asked her if she was sure he had been gay \n",
            "I had been talking and walking and talking to myself and the others  and had made no attempt to know anything about the strange things I had heard and known and fallen into a sort of reverie \n",
            "I was in love with her and was going to give it to her soon after she made it up to her \n",
            "I may wish that he had been under the tree and not his own father  but the house of his youth  and that his richest son may have been his heir \n",
            "I made a glass of wine and a jug of wine and pulled out a book of poetry and a copy of The Windbow \n",
            "I thought she was a ghost in the closet  and I thought she was a ghost in the closet \n",
            "I had to try something different with the pattern of it  but I just could nt resist it  so I just tried to make it a little more interesting \n",
            "I  the Princess  wrote to me \n",
            "I said when I was a child that there is no such thing as a ghetto  in the West\n",
            "Best Sentence:  I was in love with her and was going to give it to her soon after she made it up to her \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}