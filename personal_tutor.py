# -*- coding: utf-8 -*-
"""Personal-Tutor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ericburdett/cs673-personal-tutor/blob/master/Personal_Tutor.ipynb

# Personal Tutor

This notebook contains code for the Personal Tutor System built for CS673: Computational Creativity.

## Imports and Setup

Installing transformers requires the Runtime to be restarted on Colab. The os.kill command does that for us automatically. Just note that Colab will show an error indicating that the runtime has crashed... Ignore it and run the following code blocks
"""

# !pip install transformers
# !python -m spacy download en_core_web_md
import os
import sys
import string
# os.kill(os.getpid(), 9) # This will automatically restart the runtime. Colab will show an error... but it works

import torch
import torch.nn.functional as F
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import numpy as np
import pandas as pd
import spacy

from language import LanguageModel, Evaluator, WordDist
from user import UserKnowledge


# Download the simple word distribution from GitHub
# !wget -O word_dist_full.csv https://raw.githubusercontent.com/ericburdett/cs673-personal-tutor/master/data/word_dist_full.csv
def remove_prompt(sentence, prompt):
  new_sentence = sentence.split(prompt)
  if len(new_sentence) < 2:
    return ''
  else:
    return sentence.split(prompt)[1]

"""## Examples"""

# Pick your topic
# Set up objects for system
TOPIC = 'News'

print('Loading spacy model')
NLP = spacy.load('en_core_web_md')

print('Language model')
LM = LanguageModel(k=25)

print('Evaluator')
EVAL = Evaluator(TOPIC, NLP)

"""### Basic Model without UserKnowledge"""
# Main App Loop
NUM_SENTENCES = 10
MAX_SENTENCE_LENGTH = 40

LM.set_mask(np.ones(len(LM.tokenizer.get_vocab())))
sys.exit()

prompts = ['President Trump said']
prompts_truncate = [False]

rand_index = np.random.randint(len(prompts))
prompt = prompts[rand_index]
should_truncate = prompts_truncate[rand_index]

sentences = LM.get_sentences(prompt, MAX_SENTENCE_LENGTH, NUM_SENTENCES)
if should_truncate:
  sentences = [remove_prompt(sentence, prompt) for sentence in sentences]

scores = EVAL.score_sentences(sentences)
print('Produced Sentence: ', sentences[np.argmax(scores)])

"""### Using UserKnowledge to Modify Probability Distribution"""

# Main App Loop
NUM_SENTENCES = 10
MAX_SENTENCE_LENGTH = 40

user_knowledge = UserKnowledge(WordDist().dict_normalized(), LM.tokenizer)
# Use user_knowledge.update() to update known words -- accepts list of words and list of bools indicating known/unknown
mask = user_knowledge.compute_mask() # Run this function to compute the mask
LM.set_mask(mask) # Set the mask to the language model

prompts = ['President Trump said']
prompts_truncate = [False]

rand_index = np.random.randint(len(prompts))
prompt = prompts[rand_index]
should_truncate = prompts_truncate[rand_index]

sentences = LM.get_sentences(prompt, MAX_SENTENCE_LENGTH, NUM_SENTENCES)
if should_truncate:
  sentences = [remove_prompt(sentence, prompt) for sentence in sentences]

scores = EVAL.score_sentences(sentences)
print('Produced Sentence: ', sentences[np.argmax(scores)])

"""### Example of updating user_knowledge object and computing mask"""

# example of updating user_knowledge
user_knowledge = UserKnowledge(WordDist().dict_normalized(), LM.tokenizer)
user_knowledge.update(['the', 'the', 'a', 'from'], [True, False, True, True])
mask = user_knowledge.compute_mask()
# LM.set_mask(mask) # We can update the language model's mask with the set_mask method

# Tokens that we modified that have a higher probability of being shown
LM.tokenizer.convert_ids_to_tokens(np.squeeze(np.where(mask != 1)))
